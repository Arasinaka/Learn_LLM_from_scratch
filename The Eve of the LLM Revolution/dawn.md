# LLM革命前夜

## 机器学习分岔路

当我们拥有了神经网络后，自然而然地就想让计算机拥有人类的能力。由此，发展出两大方向

* 计算机视觉（CV，Computer Vision）, 像人一样，看世界。
* 自然语言处理（NLP，Nature Language Process），像人一样，理解语言，并用语言交流。

我们的目标是学习大语言模型（LLM），所以更加关注NLP的方向。在深入学习NLP前，我们需要先做一些技术准备。

## TBD

### MLP遇到的问题

让我们先回顾一下上文，我们用计算机模拟了人脑的生理结构和学习过程，发展出了经典的神经网络结构——多层感知机（MLP）。
MLP是一种基础的神经网络模型，它在很多问题上都表现得相当出色。然而，MLP也有一些明显的缺点：

1. 参数量大、训练难度大：MLP的网络结构中，每个神经元都和上一层中的所有节点连接，这导致参数量大，训练过程相对较耗时。
2. 丢失空间信息：MLP会丢失像素间的空间信息，只接受向量输入，这意味着，对于图像这种空间信息丰富的数据，MLP的运行模式可能不适合

为了解决这些问题，业界采取了一些策略：

1. 引入卷积神经网络（CNN）：CNN通过局部稀疏连接和接受矩阵输入的方式，利用像素间空间关系，有效地减少了参数量，同时保留了图像的空间信息
2. 使用Transformer模型：Transformer模型通过自注意力机制，可以捕获输入数据的全局依赖关系，从而提高模型的性能。

CNN和Transformer，包括后文提到的RNN/LSTM都是神经网络，就是特征提取器。站在2023年，这个时间节点，CNN解决了大多数计算机视觉邻域的问题。而Transformer，是当红炸子鸡，正在大规模的运用在NLP邻域，并且越来越多的和其他模型结合，应用到其他邻域，比如CV。
我们来了解一下

## 聚焦NLP

自然语言处理（NLP）是计算机科学、人工智能和语言学的交叉领域，主要研究如何让计算机能够理解、处理和生成人类语言。

### NLP的核心任务

1. 序列标注任务：在序列标注中，我们想对一个序列的每一个元素标注一个标签。一般来说，一个序列指的是一个句子，而一个元素指的是句子中的一个词。比如信息提取问题可以认为是一个序列标注问题，如提取出会议时间、地点等。分词/POS Tag/NER/语义标注等
2. 分类任务：例如文本分类、情感分类等
3. 句子关系判断：例如句法分析、蕴含关系判断（entailment）,自然语言推理等。
4. 生成式任务​：这类任务一般直接面向普通用户，提供自然语言处理产品服务的系统级任务，会用到多个层面的自然语言处理技术。例如机器翻译、文本摘要、总结、阅读理解、语音识别、对话系统、问答系统、自动文章分级等

第1，2点，我们也称为自然语言理解(NLU)，即理解人类语言。第3，4点，称为自然语言生成(NLG)，用来处理和生成人类的语言

## NLP的难点

1. 对标注数据的高度依赖：NLP技术对标注数据依赖性较高，难以在标注数据稀缺的任务、语言或领域内发挥作用
2. 语言的多样性：语言是没有规律的，或者说规律是错综复杂的
3. 语言的歧义性：语言是可以自由组合的，可以组合复杂的语言表达
4. 语言的鲁棒性：语言是一个开放集合，我们可以任意的发明创造一些新的表达方式
5. 语言的知识依赖：语言需要联系到实践知识，有一定的知识依赖
6. 语言的上下文：语言的使用要基于环境和上下文



