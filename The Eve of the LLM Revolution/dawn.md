# LLM革命前夜

## 机器学习分岔路

当我们拥有了神经网络后，自然而然地就想让计算机拥有人类的能力。由此，发展出两大方向

* 计算机视觉（CV，Computer Vision）, 像人一样，看世界。
* 自然语言处理（NLP，Nature Language Process），像人一样，理解语言，并用语言交流。

我们的目标是学习大语言模型（LLM），所以更加关注NLP的方向。

## 聚焦NLP

自然语言处理（NLP）是计算机科学、人工智能和语言学的交叉领域，主要研究如何让计算机能够理解、处理和生成人类语言。

### NLP的核心任务

1. 序列标注任务：在序列标注中，我们想对一个序列的每一个元素标注一个标签。一般来说，一个序列指的是一个句子，而一个元素指的是句子中的一个词。比如信息提取问题可以认为是一个序列标注问题，如提取出会议时间、地点等。分词/POS Tag/NER/语义标注等
2. 分类任务：例如文本分类、情感分类等
3. 句子关系判断：例如句法分析、蕴含关系判断（entailment）,自然语言推理等。
4. 生成式任务​：这类任务一般直接面向普通用户，提供自然语言处理产品服务的系统级任务，会用到多个层面的自然语言处理技术。例如机器翻译、文本摘要、总结、阅读理解、语音识别、对话系统、问答系统、自动文章分级等

第1，2点，我们也称为自然语言理解(NLU, Nature Language Understanding)，即理解人类语言。第3，4点，称为自然语言生成(NLG, Nature Language Generation)，用来处理和生成人类的语言。

### NLP的难点

1. 对标注数据的高度依赖：NLP技术对标注数据依赖性较高，难以在标注数据稀缺的任务、语言或领域内发挥作用
2. 语言的多样性：语言是没有规律的，或者说规律是错综复杂的
3. 语言的歧义性：语言是可以自由组合的，可以组合复杂的语言表达
4. 语言的鲁棒性：语言是一个开放集合，我们可以任意的发明创造一些新的表达方式
5. 语言的知识依赖：语言需要联系到实践知识，有一定的知识依赖
6. 语言的上下文：语言的使用要基于环境和上下文

要让计算机理解人类的语言，第一步就是把语言表示成计算机认识的符号。
在深入学习NLP前，我们需要先做一些技术准备。NLP 的技术基础方面，我认为主要是这两部分：

1. 词表示法（Word Representation）
2. 语言模型（Language Model）

我会简单的介绍一下词表示法中的Word Embedding，而把重心放在讲解语言模型这部分。

## Transformer之前的网络模型

上一篇文章，我们介绍了感知机和MLP。我们先做一点扩展阅读。

### 多层感知网络（MLP）

![NN_MLP.svg](assets/NN_MLP.svg)

### 卷积神经网络（CNN）

卷积神经网络（CNN）是为了解决前馈神经网络在处理图像或其他网格型数据（如二维的图像或三维的视频数据）时的缺点而发明的。

前馈神经网络的一个主要限制是它们对输入数据的空间结构没有直观的理解。例如，当处理图像数据时，前馈神经网络会将图像展平成一个长向量，从而丢失了图像的空间结构信息（即像素之间的相对位置信息）。这使得前馈神经网络在处理图像等网格型数据时效率低下，因为它们无法有效地利用这些数据的局部性质。

相比之下，CNN通过使用卷积层来处理输入数据，可以保留并利用数据的空间结构信息。卷积层通过在输入数据上滑动小的窗口（或称为“卷积核”或“滤波器”）来工作，这使得CNN能够捕捉到局部的空间特征。此外，CNN还使用了池化层（pooling layers）来降低数据的空间维度，从而减少计算量并提高模型的泛化能力。

因此，CNN在处理图像识别、物体检测、语义分割等任务时表现出了优越的性能，这些任务都需要对输入数据的空间结构进行理解。

### 循环神经网络（RNN）

循环神经网络（RNN）是为了解决前馈神经网络在处理序列数据时的缺点而发明的。

前馈神经网络的一个主要限制是它们假设输入数据是独立的，这意味着它们不能处理输入数据之间的时间或空间关系。例如，当处理一段文本或一段音频时，前馈神经网络无法理解单词或音符之间的顺序。

相比之下，RNN通过在网络中添加循环连接，可以记住过去的信息，从而能够处理序列数据。这使得RNN能够理解文本、音频、时间序列数据等的顺序信息，因此在许多任务中，如语音识别、自然语言处理、时间序列预测等，RNN都表现出了优越的性能。

如何理解RNN呢？我们先把上面那个MLP图，做一下简化和旋转90度的变形，如下图所示：

![NN_RNN_1.svg](assets/NN_RNN_1.svg)

为了表示时序性，我们需要把多个这样的结构串联起来，即用激活函数(比如tanh函数）和权重把每个单元的连接，如下图所示：
![NN_RNN_2.svg](assets/NN_RNN_2.svg)

看到这个图，大家有没有觉得眼熟？像不像数据结构里的链表？它也有单链表的麻烦，第一个节点S1和其他节点不一样。我们也沿用对于链表的加入首结点的思路来处理。我们给S1加入一个虚拟输入并令权重为0，这样我们把RNN图优雅的表示成一个循环图，如下图所示：
![NN_RNN_3.svg](assets/NN_RNN_3.svg)

接下来我们用数学公式把RNN表示出来，先把RNN图展开。

![NN_RNN_4.svg](assets/NN_RNN_4.svg)
由MLP的相关知识，我们可以把$S_t$ 和 $O_t$表示为：

$$
O_t = g(V \cdot s_t + b_2) \\
S_t = f(U \cdot x_t + W \cdot s_{t-1} + b_1)
$$

通过两个式子循环推到，很容易得到：

$$
\begin{aligned}
O_t & = g(V \cdot s_t + b_2) \\
& = g(V \cdot f(U \cdot x_t + W \cdot s_{t-1} + b_1) \\
& = g(V \cdot f(U \cdot x_t + W \cdot f(U \cdot x_{t-1} + W \cdot S_{t-2} + b_1) + b_2) \\
& = g(V \cdot f(U \cdot x_t + W \cdot f(U \cdot x_{t-1} + W \cdot f(U \cdot x_{t-2} + ...) + b_1) + b_2)  \\
\end{aligned}
$$

可以看到，当前时刻的输出包含了历史信息，这说明循环神经网络对历史信息进行了保存。
这里有几点需要注意：

* 你可以将隐藏的状态$s_t$看作网络的记忆，它捕获有关所有先前时间步骤中发生的事件的信息。步骤输出$o_t$根据时间t的记忆计算。正如上面简要提到的，它在实践中有点复杂，因为$s_t$通常无法从太多时间步骤中捕获信息。
* 与在每层使用不同参数的传统深度神经网络不同，RNN共享相同的参数（所有步骤的$U,V,W$）。这反映了我们在每个步骤执行相同任务的事实，只是使用不同的输入，这大大减少了我们需要学习的参数总数。

上图在每个时间步都有输出，这种输入和输出数据项数一致的 RNN，一般叫做 N：N 的 RNN。但根据任务，这可能不是必需的。例如，在预测句子的情绪时，我们可能只关心最终的输出，而不是每个单词之后的情绪。同样，我们可能不需要在每个时间步骤输入。所以，RNN结构可以是不同的组合，下图是N：1型和1:N型的RNN。N:M型的，我们放在后续章节再讨论。

![NN_RNN_5.svg](assets/NN_RNN_5.svg)

### 长短时记忆网络（LSTM）

我们回顾一下RNN的特点——​可以有效的处理序列数据。RNN把每个时刻都会把隐藏层的值存下来，到下一时刻的时候再拿出来用，这样就保证了，每一时刻含有上一时刻的信息。换句话说，我们用RNN模拟出了人脑的记忆功能。问题来了，RNN把所有信息，包括无用的信息，都等权重的保存下来了。显然，这种模拟比起人脑来，显得很是初级。
LSTM（长短期记忆）是一种特殊的RNN（循环神经网络），LSTM的设计借鉴了人类对于自然语言处理的直觉性经验。在一个时间序列中，不是所有信息都是同等有效的，大多数情况存在“关键词”或者“关键帧”。我们会在从头到尾阅读的时候“自动”概括已阅部分的内容并且用之前的内容帮助理解后文。简单来说，就是相比普通的RNN，LSTM能够在更长的序列中有更好的表现。
LSTM的设计者提出了“长短期记忆”的概念——只有一部分的信息需要长期的记忆，而有的信息可以不记下来。同时，我们还需要一套机制可以动态的处理神经网络的“记忆”，因为有的信息可能一开始价值很高，后面价值逐渐衰减，这时候我们也需要让神经网络学会“遗忘”特定的信息。



LSTM内部主要有三个阶段：

1. 忘记阶段。这个阶段主要是对上一个节点传进来的输入进行选择性忘记。简单来说就是会 “忘记不重要的，记住重要的”。具体来说是通过计算得到的 $z_f$ （f表示forget）来作为忘记门控，来控制上一个状态的 $c_{t-1}$ 哪些需要留哪些需要忘。
2. 选择记忆阶段。这个阶段将这个阶段的输入有选择性地进行“记忆”。主要是会对输入 $x_t$ 进行选择记忆。哪些重要则着重记录下来，哪些不重要，则少记一些。当前的输入内容由前面计算得到的$z$ 表示。而选择的门控信号则是由 $Z_i$ （i代表input）来进行控制。

> 将上面两步得到的结果相加，即可得到传输给下一个状态的$c_t$ 。也就是上图中的第一个公式。

3. 输出阶段。这个阶段将决定哪些将会被当成当前状态的输出。主要是通过 $Z_o$(o代表output) 来进行控制的。并且还对上一阶段得到的 $c_o$ 进行了放缩（通过一个tanh激活函数进行变化）。

与普通RNN类似，输出 $o_t$ 往往最终也是通过 $h_t$ 变化得到。


