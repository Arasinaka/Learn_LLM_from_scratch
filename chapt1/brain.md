# 如何让计算机模拟人脑工作

在开始写《从零开始学大语言模型LLM》前，我构思了很久如何动笔。回顾这2个月的学习历程，感觉有三次不经意的对话，却让我受益匪浅。于是乎，决定从三次对话开始写。

> 来自2023年8月的一次对话
> 
> > Carson：最近看了几篇AIGC的相关论文，比如Attention is all your needs，也大致也明白这些算法是如何工作的。但是我有一个疑惑，就是这些人是怎么想到这些方法的？
> > 阿甘：现代AI算法都是在模拟人脑，发现模拟的越来越像，效果越好。

## 算法，数据结构和人脑神经网络结构

为了对比计算机和人脑解决问题，我们先把算法（Algorithm）赋予更抽象的概念。

> **算法**：指解决问题的完整描述，由一系列准确可执行的步骤组成，其代表着解决问题的策略。

我们先来回顾一下计算机程序是如何工作的？

* 算法由程序描述，程序被转化成指令，指令被硬件（逻辑门结构）执行，这就实现了数据的逻辑运算。
* 算法本身却由人脑（也称人类智能）创造的，并通过算法控制计算机完成逻辑推理。

在此我们会发现，人脑可以构造算法，但计算机却不行，而算法才是逻辑推理的关键，那么这其中的奥秘是什么呢？

答案——结构

事实上，计算机的存储结构、传输结构与计算结构是独立分离的，但人脑神经网络结构，三者是合为一体的。因此，数据+算法，会存在于同一个**脑结构**之中。

如前所述，能够创造出算法是智能的关键所在，而在编程领域，著名程序员、开源软件运动的思想家、黑客文化的理论家——埃里克·雷蒙德（Eric Raymond），在《Unix编程艺术》中，有这样一个实践性的洞见——**算法**和**数据结构**有一个关系，即：

> 数据结构越复杂（如哈希表），算法就可以越简单，数据结构越简单（如数组），那么算法就需要越复杂。

例如，编程语言越是动态化（如Python、JavaScript），就越容易构建复杂结构，用其编写算法也就越容易，相反编程语言越是静态化（如C、C++、Java），就越难以构建复杂结构，用其编写算法就困难，而编程语言的演化是越来越动态化（如C#）。

为什么算法和数据结构有这样一个关系呢？其原理就在于，

* **算法**是逻辑关系的“计算映射”，即动态地进行逻辑关系的转化；
* **数据结构**是逻辑关系的“固化映射”，即将已经计算好的逻辑关系，存储在了结构之中。

可见，算法比数据结构多出了计算的过程——前者需要先计算后读写，后者仅需要根据结构的逻辑关系直接读写。所以，用数据结构进行逻辑关系的转化，会更加高效。
再来看一下人脑，人脑可以从环境信息中，提取数据结构并习得算法，最终将两者存储到脑结构之中。

这样我们得到一个结论，

> **神经结构、数据结构、算法**三者之间可以互相转化，或说互相表征。

换言之，如果数据结构足够强大，它就可以充当复杂算法的功能，甚至可以替代复杂的神经结构。

因此，计算机智能拟人（即模拟人脑）的一个途径，就是通过强化数据结构来模拟神经结构，以及弱化人类智能所提供的代码实现的算法，转而使用数据结构去生成算法，而这就是目前人工智能的发展方向。

至此，我得出了学习人工智能/机器学习最重要的两大关键。

1. 网络结构
2. 模拟人类

## 神经元的工作原理

### 神经元结构

我们从最简单的开始，先弄明白人体的神经元 ，先看一下人体神经元的结构图。
![Neuron_Handtunedsvg.png](assets/Neuron_Hand-tuned.svg.png)

**神经元**（Neuron），是组成神经系统结构和执行神经功能活动的一大类高度分化细胞，由胞体和胞突（树突和轴突）组成，属神经组织的基本结构和功能单位。神经元大致分为三类：感觉（传入）神经元，运动（传出）神经元，联络（中间）神经元。不论是何种神经元，皆可分成：

1. 接收区（receptive zone）
2. 触发区（trigger zone）
3. 传导区（conducting zone）
4. 输出区（output zone）

### 神经元工作过程

工作过程：其他神经元的信号（输入信号）通过树突传递到细胞体，细胞体把其他多个神经元传递过来的输入信号进行合并加工，然后通过轴突（输出信号）传递给别的神经元。

来自Wikipedia的解释，该过程专业术语叫**动作电位**（action potential），指的是静止膜电位状态的细胞膜受到适当刺激而产生的，短暂而有特殊波形的跨膜电位搏动。细胞产生动作电位的能力被称为兴奋性，有这种能力的细胞如神经细胞和肌细胞。动作电位是实现神经传导和肌肉收缩的生理基础。
一个初始刺激，只要达到了**阈电位**（threshold potential），不论超过了多少，也就是全有全无律，就能引起一系列离子通道的开放和关闭，而形成离子的流动，改变跨膜电位。

综上所示，整个过程有5个重要的概念

1. 输入
2. 对输入信号的处理
3. 阈值
4. 激活（超过阈值，引起通道的开发和关闭）
5. 输出

现在，我们画一个模拟图，来模拟神经元工作过程。

![neuron_flow.svg](assets/neuron_flow.svg?t=1700127147569)

## 用数学来表示神经元

在开始这一章节前，先把时间拨回到2017年。

> 来自2017年的一次办公室闲聊
> 
> > Carson：最近计算机业界出了几个很火的概念，我周末花了点时间看了一下，结果感觉就是旧瓶装新酒……
> > 杨博士：计算机本身不难，难的是数学。
> > Carson：噢，数学……

至那以后，我每每遇到工作和学习的难题时，都会自然而然的用数学思维来思考。原先一些很难掌握的知识点，比如函数式编程，就直接秒懂了。这种感觉很妙，你也不妨试试……

言归正传，现在我们将上文终结出来的5个概念，即输入，处理，阈值，激活和输出，用数学表示出来。

参考我们熟悉的一次函数，我们用$x_i$表示输入；用Y表示输出。因为有多路输入，所以我们还要引入权重（Weight）这个概念。对于输入的处理，就对应数学里的函数$f(x)$，我们一般用线性方程，比如求和$\sum$表示。阈值，我们用一个常量bias表示，简写为$b$。激活，也对应于数学中的函数，我们称之为激活函数（Activation Function）,我们用$\sigma$来表示，一般是非线性函数，比如ReLU，Sigmod等函数。

那么，我们就得到了下图。

![M_P.png](assets/M_P.png)
其实这个图就是McCulloch和Pitts在1943年提出来的M-P神经元模型，并由此模型发展出了感知机（Preceptron）。而这个从左到右，即从输入到输出的过程，在机器学习里的专业术语叫**前向传播**（Forward Propagation）。整个过程，我们只用一个数学就可以表示出来：

$$
Y = \sigma(\sum_{i=1}^n w_ix_i + b)
$$

### 番外篇1：Bias与Activation Function的区别

在我学习Machine Learning的时候，一开始对这两个概念有一定的困惑。如果你遇到和我一样的问题，就请问问ChatGPT吧。

> Carson: 在计算机神经网络中，如何区分bias和activation function

> ChatGPT: 在计算机神经网络中，bias和activation function是两个非常重要的概念，它们对神经网络的性能和准确性有很大的影响，但是它们的作用是不同的。
> Bias是神经网络中的一种参数，也被称为偏置，它与权重一样，在神经网络的训练过程中会被优化。Bias通常表示为一个浮点数或向量，它的作用是为神经网络的每个神经元提供一个常数偏移量，帮助神经元更好地学习输入数据的模式。每个神经元都有自己的bias，作为输入信号的一部分被传递给神经元的激活函数。 在数学上，bias可以看作是一种向输入信号添加常数偏移量的方法，使神经元更容易地学习输入数据中具有偏差的模式。
> Activation function（激活函数）是神经网络中的一个函数，它的作用是为神经元提供输出，并将输入信号转换为输出。在神经网络中，神经元的激活函数通常是一种非线性函数，例如sigmoid函数、tanh函数或ReLU函数等。这些函数的作用是将神经元的输出限制在特定范围内，使得神经元能够学习和表示更加复杂的函数关系。Activation function将输入与与bias相加后的结果进行变换，将结果转换为输出。
> 因此，Bias和Activation function在计算机神经网络中虽然有些相似的用途，但是它们的作用是不同的。Bias是用来帮助神经元更好地学习输入数据，而激活函数是用来限制神经元的输出并实现非线性变换的。

至此，我们已经用计算机构建出最简单的神经元了。有了单个神经元，就能将多个神经元练成神经网络，这样我们就用计算机模拟出了人脑结构。
现在我们已经计算机模拟出了人脑，如同一个刚刚出生的婴儿。婴儿要长大，就开开始学习啦。接下来，我们来谈谈学习这件事情。

## 人脑是如何运作的

人脑的运作原理，简单地讲，它是这样运作的（摘自知乎）：

人类通过感知系统(比如视觉，听觉等)输入信息，输入的信息不断地被大脑神经元系统并行地处理，层层向上抽象，整个抽象过程是基于大脑特定的结构设计决定的，不是一团没有设计的神经元混沌，而是一条有规则的组合关系。信息不断地流入，模式提取和预测奖惩同时运作，分布式的神经元激活指代着各种实体以及实体间的关系。运用已经观察到的实体及关系，我们进行着新的学习和预测。一旦，遇到新鲜的模式，我们就会重构已有的记忆以熵最小化的形式进行记忆，新的记忆指导新的行动，并帮助你理解新的事物。

上述文字信息量还是比较大的，我们来讨论提到几个重要的过程。

1. 感知系统，处理视觉，听觉等输入信息。对应**数据采集**
2. 对信息的抽象和提取。对应**数据编码**
3. 把提取的信息，按照规则组合关系激活分布式神经元，建立神经元连接。对应**数据存储**
4. 遇到新模式，重构已有记忆（即重构神经元连接）帮助理解新事物。对应**数据检索**
5. 学习的过程需要很多轮，同时还要有奖惩机制，即多巴胺机制。

### 用数学来表示学习的过程

先回顾一下感知机的构成，有输入$x_i$;权重$w_i$;函数$\sum$;阈值$b$;激活函数$\sigma$;输出$Y$。然后结合上文的5个过程，我们很容易就能想到用输入$x_i$对应数据采集。而数据编码即数据存储不是那么一目了然。但是，我们仔细看一下感知机，函数$\sum; \sigma$一旦选定就不变了，只有权重$w_i$和阈值$b$是可以随意改变，并可以保存。
瞧，数学总是这么一目了然，至此我们找到了机器学习的本质：

> 机器学习本质就是用各种各样的的数学手段，来调整权重(Weight)

Tips: 在实际应用中，我们把阈值$b$，直接写作$w_b$，也作为权重进行训练。本文不区分这两个概念，这里我们更加侧重学习原理。

#### 损失函数

还有最后一个过程我们需要用数学来表示，多轮的学习和奖惩机制。
回顾一下当你学习的时候，大脑会发生重要变化。即在神经元之间建立新连接——这种现象被称为神经可塑性(Neuroplasticity)。练习得越多，这些连接就会变得越牢固。当连接加强时，信息(神经冲动)的传输速度越来越快，也更有效率。这里，我们用权重来表示神经元的连接强度。
谈到奖惩机制，我们首先要解决的就是输出$Y$是不是就是“正确答案”。对比输出$Y$是否接近“正确答案”这一过程，类似于人类学习中的考试，分数越高表示越接近正确答案。
举个例子，在满分为100分的考试中，得分98分的学生A，比得分为85分的学生B，更应该收到奖励。
我们做一个等式变形，

* 学生A离目标分数100分的距离$J(\theta_a) =\vert 98-100 \vert = 2$
* 学生B离目标分数100分的距离$J(\theta_b) = \vert 85-100 \vert = 15$
  显然这里的距离$J(\theta)$越小越好。在机器学习中，这个距离函数，我们称之为**损失函数**（Loss Function)。

就如图灵奖得主、卷积神经网络之父——杨立昆（Yann LeCun），在《科学之路》中，所说：

> 所谓的机器学习，就是机器进行尝试、犯错和自我调整的操作。学习就是逐步减少系统误差的过程。训练机器的过程就是调整参数的过程。……基于损失函数最小化的学习，是人工智能运作的关键要素。通过调整系统参数来降低损失函数，也就是降低实际输出与期望输出之间的平均误差。实际上，最小化损失函数和训练系统是一回事。

#### 梯度下降和反向传播

我们已经知道训练的过程，就是使得损失函数最小化的过程，需要用到最优化算法。一般而言，损失函数很复杂，参数很多，很难用数学方程直接求得最小值。我们往往采用无限逼近的办法求得近似值，这个数学方法正式名称叫**梯度下降**。

梯度下降和反向传播是机器学习中常用的优化算法。梯度下降是一种迭代算法，用于最小化损失函数。它通过计算损失函数相对于模型参数的梯度来更新模型参数。反向传播是一种计算梯度的方法，它使用链式法则来计算损失函数相对于每个模型参数的梯度。这些梯度可以用于更新模型参数，以便最小化损失函数。

梯度下降算法的数学原理是，通过计算损失函数相对于模型参数的梯度来更新模型参数。梯度是一个向量，它指向损失函数增加最快的方向。因此，通过沿着梯度的反方向更新模型参数，可以最小化损失函数。梯度下降算法的更新规则如下：

$$
\theta_{t+1} = \theta_t - \eta \nabla{J(\theta_t)}
$$

其中，$\theta_t$是第$t$次迭代后的模型参数，$eta$是学习率，$\nabla{J(\theta_t)}$ 是损失函数 $J$相对于模型参数$\theta_t$的梯度。

反向传播算法的数学原理是，使用链式法则计算损失函数相对于每个模型参数的梯度。链式法则是一种计算复合函数导数的方法。在神经网络中，每个层都可以看作是一个函数，它将输入映射到输出。通过使用链式法则，可以计算出损失函数相对于每个层的输入和权重的梯度。这些梯度可以用于更新模型参数，以便最小化损失函数。

链式法则是微积分中的一种求导法则，用于求解复合函数的导数。当一个函数是由两个或多个函数组成时，链式法则可以帮助我们找到这个复合函数的导数。链式法则的正式表述如下：设 $y=f(u), u=g(x)$ ，则 $y$ 关于 $x$ 的导数为$\frac{dy}{dx} = \frac{dy}{du} \cdot \frac{du}{dx}$。这个公式可以推广到多元函数的情况。例如，如果 $z=f(x,y), x=g(t), y=h(t)$，则$z$关于$t$的导数为$\frac{dz}{dt} = \frac{\partial z}{\partial x} \cdot \frac{dx}{dt} + \frac{\partial z}{\partial y} \cdot \frac{dy}{dt}$。这个公式被称为多元复合函数求导法则。

### 番外篇2：记忆和预测的区别

从认知心理学的角度来看，记忆和预测的机理在某些方面是相似的，但也存在一些重要的差异。

记忆是指保存和提取过去经验和信息的能力。在认知心理学中，记忆被分为多个阶段，包括感知、编码、存储和检索。感知阶段涉及将外部刺激转化为神经信号，编码阶段将信息转化为可存储的形式，存储阶段将信息存储在长期记忆中，检索阶段则是从存储中提取信息。这种过程使得我们能够回忆起过去的事件、事实和经验。

预测是指根据已有的信息和经验，对未来事件或情况进行估计或猜测的能力。在认知心理学中，预测涉及对已有知识和模式的识别、提取和应用。通过将过去的经验与当前的环境进行比较，我们可以推测出可能发生的未来事件。这种能力使我们能够做出决策、规划行动和适应新的环境。

尽管记忆和预测在某些方面存在相似之处，但也存在重要的差异。记忆主要关注过去事件和经验的存储和提取，而预测则侧重于基于已有信息推断未来可能发生的情况。记忆更多地涉及回溯和回顾，而预测则更侧重于前瞻和推断。

## 小结

## 参考文献

